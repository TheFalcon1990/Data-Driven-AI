{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UqNAHL7417nP"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "hhh-tfOkDYWL"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T6T6btY15iU"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nARc2I1BAchK"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download asl_dataset from Brightspace and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmmZvzmJAchM"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folder_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading images from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfolder_path\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'folder_path' is not defined"
     ]
    }
   ],
   "source": [
    "img_size = (150, 150,3)\n",
    "\n",
    "sign_0 = glob.glob('./asl_dataset/0/*.*')\n",
    "sign_1 = glob.glob('./asl_dataset/1/*.*')\n",
    "sign_2 = glob.glob('./asl_dataset/2/*.*')\n",
    "sign_3 = glob.glob('./asl_dataset/3/*.*')\n",
    "sign_4 = glob.glob('./asl_dataset/4/*.*')\n",
    "sign_5 = glob.glob('./asl_dataset/5/*.*')\n",
    "sign_6 = glob.glob('./asl_dataset/6/*.*')\n",
    "sign_7 = glob.glob('./asl_dataset/7/*.*')\n",
    "sign_8 = glob.glob('./asl_dataset/8/*.*')\n",
    "sign_9 = glob.glob('./asl_dataset/9/*.*')\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in sign_0:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(0)\n",
    "for i in sign_1:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(1)\n",
    "for i in sign_2:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(2)\n",
    "for i in sign_3:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(3)\n",
    "for i in sign_4:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(4)\n",
    "for i in sign_5:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(5)\n",
    "for i in sign_6:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(6)\n",
    "for i in sign_7:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(7)\n",
    "for i in sign_8:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(8)\n",
    "for i in sign_9:\n",
    "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= img_size)\n",
    "    image=np.array(image)\n",
    "    data.append(image)\n",
    "    labels.append(9)\n",
    "\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in sign_0: []\n",
      "Files in sign_1: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Files in sign_0:\", sign_0)\n",
    "print(\"Files in sign_1:\", sign_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coYf31ZiEpvR"
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kLXsAXV8EpKC"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m786\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal training intences: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_train)))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n",
      "File \u001b[1;32mc:\\Users\\hussa\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hussa\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hussa\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=786, stratify=labels)\n",
    "print(\"Total training intences: \" + str(len(y_train)))\n",
    "print(\"Train Data:\" + str(np.unique(y_train, return_counts=True)))\n",
    "print(\"Total testing intences: \" + str(len(y_test)))\n",
    "print(\"Test Data:\" + str(np.unique(y_test, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRZjH2b9MVbg"
   },
   "source": [
    "## Reshape Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IQZ-4IFMViV"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\nBefore Reshaping the shape of train and test dataset:\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape((X_train.shape[0], 150, 150, 3))\n",
    "X_test = X_test.reshape((X_test.shape[0], 150, 150, 3))\n",
    "print(\"After Reshaping the shape of train and test dataset:\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1_aYlCyAchQ"
   },
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-PunA0dAchQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(40,40))\n",
    "\n",
    "num_images = 20\n",
    "for i in range(num_images):\n",
    "    row = X_train[i]\n",
    "    label = y_train[i]\n",
    "\n",
    "    image = row\n",
    "    plt.subplot(1, num_images, i+1)\n",
    "    plt.title(label, fontdict={'fontsize': 30})\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWDmhHF-MeMO"
   },
   "source": [
    "## Normalize the Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyJXxemfMeUF"
   },
   "outputs": [],
   "source": [
    "# Convert the array to float32 as opposed to uint8\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Convert the pixel values from integers between 0 and 255 to floats between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /=  255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qJeoK8SMkjF"
   },
   "source": [
    "## Categorize/Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4efqwHgmMknW"
   },
   "outputs": [],
   "source": [
    "# Number of classes in your dataset\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "print(\"Before label conversion to categorical: \", y_train[0:5]) # The format of the labels before conversion\n",
    "y_train  = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "\n",
    "print(\"After label conversion to categorical: \", y_train[0:5]) # The format of the labels after conversion\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffQESA7O15id"
   },
   "source": [
    "## Creating a Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-cazfIn15id"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5, 5), strides=1, padding=\"same\", activation=\"relu\", input_shape=img_size))\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(32, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation=\"relu\"))\n",
    "model.add(Dense(units=NUM_CLASSES, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_hwLiET15il"
   },
   "source": [
    "## Summarizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpydN8-A15il"
   },
   "source": [
    "Here, we summarize the model we just created. Notice how it has fewer trainable parameters than the model in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JzJgA9W15il"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoUSJasG15im"
   },
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2gRs5JN15im"
   },
   "source": [
    "We'll compile the model just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd-xZBxW15im"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCvkmxGb15im"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8K0fPta15im"
   },
   "source": [
    "Despite the very different model architecture, the training looks exactly the same. Run the cell below to train for 20 epochs and let's see if the accuracy improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57y0ChtS15in"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, verbose=1, validation_split=0.3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTuB7BmG15in"
   },
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Vk-iINqOAPS"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M09jhlowwJuO"
   },
   "source": [
    "__Model Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfL5XhG_wJuO"
   },
   "outputs": [],
   "source": [
    "#### testing model on unseen test images\n",
    "y_pred = model.predict(X_test)\n",
    "#Convert class attribute back to its origional form, 0,1 from categorical [1. 0., 0. 1.]\n",
    "y_actual = np.argmax(y_test,axis=1)\n",
    "print(y_actual[0:25])\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "print(y_pred[0:25])\n",
    "\n",
    "#Accuracy of the model on test dataset\n",
    "print(classification_report(y_actual,y_pred))\n",
    "print(confusion_matrix(y_actual,y_pred))\n",
    "print(accuracy_score(y_actual,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz41FYcYONCx"
   },
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWrLdXGDOP6K"
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'], label=\"Training\")\n",
    "plt.plot(history.history['val_accuracy'], label =\"Validation\")\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'], label=\"Training\")\n",
    "plt.plot(history.history['val_loss'], label =\"Validation\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhBIB8lcOYqr"
   },
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPcz2rfq7Xi6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(filename, model):\n",
    "    img_ = image.load_img(filename, target_size=img_size)\n",
    "    img_array = image.img_to_array(img_)\n",
    "    img_processed = np.expand_dims(img_array, axis=0)\n",
    "    img_processed /= 255.\n",
    "\n",
    "    prediction = model.predict(img_processed)\n",
    "\n",
    "    index = np.argmax(prediction)\n",
    "\n",
    "    plt.title(\"Prediction - {}\".format(str(index).title()), size=18, color='red')\n",
    "    plt.imshow(img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x481OrQi7XlT"
   },
   "outputs": [],
   "source": [
    "predict_image('./asl_dataset/3/hand1_3_bot_seg_4_cropped.jpeg', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx5gZ54yQkaI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
